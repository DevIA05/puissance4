{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1f46b-859b-4b92-b708-5735aead6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Créez un environnement Puissance 4 personnalisé à l'aide de Gym\n",
    "class ConnectFourEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((6, 7), dtype=int)  # Plateau de jeu de 6x7\n",
    "        self.current_player = 1  # Joueur 1 commence\n",
    "        self.action_space = gym.spaces.Discrete(7)  # 7 colonnes possibles\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=2, shape=(6, 7), dtype=int)\n",
    "        self.winner = None  # Stocke le vainqueur\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((6, 7), dtype=int)\n",
    "        self.current_player = 1\n",
    "        self.winner = None\n",
    "        return self.board\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.winner is not None:\n",
    "            return self.board, 0, True, {}  # Le jeu est terminé\n",
    "\n",
    "        # Vérifiez si la colonne est valide\n",
    "        if self.board[0, action] != 0:\n",
    "            return self.board, -1, False, {}  # Coup invalide, pénalité\n",
    "\n",
    "        # Placez le jeton dans la colonne\n",
    "        for row in range(5, -1, -1):\n",
    "            if self.board[row, action] == 0:\n",
    "                self.board[row, action] = self.current_player\n",
    "                break\n",
    "\n",
    "        # Vérifiez s'il y a un gagnant\n",
    "        self.winner = self.check_winner()\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if self.winner is not None:\n",
    "            if self.winner == 1:\n",
    "                reward = 1\n",
    "            elif self.winner == -1:\n",
    "                reward = -1\n",
    "            done = True\n",
    "        elif np.count_nonzero(self.board) == 42:\n",
    "            done = True\n",
    "\n",
    "        # Changez de joueur\n",
    "        self.current_player *= -1\n",
    "\n",
    "        return self.board, reward, done, {}\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Vérifiez s'il y a un gagnant\n",
    "        for player in [1, -1]:\n",
    "            for row in range(6):\n",
    "                for col in range(7):\n",
    "                    if (\n",
    "                        self.board[row, col] == player\n",
    "                        and self.check_direction(row, col, player, 1, 0)\n",
    "                        or self.check_direction(row, col, player, 0, 1)\n",
    "                        or self.check_direction(row, col, player, 1, 1)\n",
    "                        or self.check_direction(row, col, player, -1, 1)\n",
    "                    ):\n",
    "                        return player\n",
    "        return None\n",
    "\n",
    "    def check_direction(self, row, col, player, dr, dc):\n",
    "        for i in range(3):\n",
    "            if (\n",
    "                row + i * dr < 0\n",
    "                or row + i * dr >= 6\n",
    "                or col + i * dc < 0\n",
    "                or col + i * dc >= 7\n",
    "                or self.board[row + i * dr, col + i * dc] != player\n",
    "            ):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "# Définissez une politique simple pour votre agent (Q-Learning)\n",
    "def q_learning_agent(env, num_episodes=10000, learning_rate=0.1, discount_factor=0.99, epsilon=0.1):\n",
    "    Q = np.zeros((6, 7, 7))  # Tableau Q (état, action)\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()  # Exploration aléatoire\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Mise à jour de la table Q\n",
    "            Q[state][action] = (1 - learning_rate) * Q[state][action] + learning_rate * (\n",
    "                reward + discount_factor * np.max(Q[next_state])\n",
    "            )\n",
    "            state = next_state\n",
    "    return Q\n",
    "\n",
    "# Entraînez l'agent\n",
    "env = ConnectFourEnv()\n",
    "Q = q_learning_agent(env)\n",
    "\n",
    "# Fonction pour jouer contre l'IA\n",
    "def play_vs_agent(Q):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        if env.current_player == 1:\n",
    "            action = int(input(\"Votre tour (0-6) : \"))\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "            print(f\"L'IA joue {action}\")\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "    env.render()\n",
    "    if env.winner == 1:\n",
    "        print(\"Vous avez gagné !\")\n",
    "    elif env.winner == -1:\n",
    "        print(\"L'IA a gagné !\")\n",
    "    else:\n",
    "        print(\"Match nul !\")\n",
    "\n",
    "# Jouez contre l'IA\n",
    "play_vs_agent(Q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
